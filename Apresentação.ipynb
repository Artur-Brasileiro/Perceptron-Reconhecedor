{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a072937",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo\n",
    "\n",
    "### 1. Importação das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fa20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c51e2",
   "metadata": {},
   "source": [
    "### 2. Configuração do Dispositivo\n",
    "\n",
    "Verifica se há GPU disponível e informa qual dispositivo será usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff356acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf06a6d",
   "metadata": {},
   "source": [
    "### 3. Transformações das Imagens\n",
    "\n",
    "Cada imagem do EMNIST tem tamanho 28x28.\n",
    "Transformamos em tensor e achatamos em um vetor de 784 valores (28×28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e57422",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3aceac",
   "metadata": {},
   "source": [
    "### 4. Carregamento do Dataset EMNIST Letters\n",
    "\n",
    "A base EMNIST Letters contém letras manuscritas.\n",
    "Aqui carregamos os dados de treino e teste, já aplicando as transformações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccfbb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.EMNIST(root='data', split='letters', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.EMNIST(root='data', split='letters', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2878cb",
   "metadata": {},
   "source": [
    "### 5. Conversão para Problema Binário (A vs Não-A)\n",
    "\n",
    "O dataset original possui 26 classes (A–Z).\n",
    "Transformamos em um problema binário:\n",
    "\n",
    "   * 1 → letra A\n",
    "\n",
    "   * 0 → qualquer outra letra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = torch.where(train_dataset.targets == 1, 1, 0)\n",
    "test_targets = torch.where(test_dataset.targets == 1, 1, 0)\n",
    "train_dataset.targets = train_targets\n",
    "test_dataset.targets = test_targets\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b4c78",
   "metadata": {},
   "source": [
    "### 6. Definição do Modelo Perceptron\n",
    "\n",
    "Um Perceptron simples com:\n",
    "\n",
    "   * Entrada: 784 neurônios (28×28 pixels)\n",
    "\n",
    "   * Saída: 1 neurônio (resultado binário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b369bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(28*28, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = Perceptron().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d258ebe",
   "metadata": {},
   "source": [
    "### 7. Correção de Desbalanceamento\n",
    "\n",
    "Como a letra “A” aparece menos vezes, aplicamos peso maior para a classe positiva (A)\n",
    "usando pos_weight na função de perda BCEWithLogitsLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = torch.tensor([4.0]).to(device) \n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1d439",
   "metadata": {},
   "source": [
    "### 8. Treinamento do Modelo\n",
    "\n",
    "Treinamos por 30 épocas, calculando a perda média em cada uma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9502e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando o treinamento (v5.0 - Mais Calmo)...\")\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f\"Época {epoch+1}/30 - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Treinamento concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa0fd3",
   "metadata": {},
   "source": [
    "### 9. Avaliação no Conjunto de Teste\n",
    "\n",
    "Usamos o modelo para prever as classes e calcular:\n",
    "\n",
    "   * Acurácia\n",
    "\n",
    "   * Relatório de classificação (precision, recall, f1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.float().to(device)\n",
    "        outputs = torch.sigmoid(model(images)).squeeze()\n",
    "        preds = (outputs > 0.5).float()\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nPrecisão final no set de teste (v5.0): {acc*100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Não-A\", \"A\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2149983",
   "metadata": {},
   "source": [
    "### 10. Salvando o Modelo Treinado\n",
    "\n",
    "O modelo é salvo em arquivo .pth para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'perceptron_A_v5.pth')\n",
    "print(\"\\nModelo treinado (v5.0) salvo como 'perceptron_A_v5.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49ca05",
   "metadata": {},
   "source": [
    "# Perceptron Funcionando\n",
    "\n",
    "### 1. Importações das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993de2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Bibliotecas de IA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Bibliotecas de Imagem\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO \n",
    "\n",
    "# Imports de QWidget e Layouts (Ignorados na explicação do modelo)\n",
    "from PyQt5.QtWidgets import (QApplication, QWidget, QPushButton, QVBoxLayout, QHBoxLayout, \n",
    "                             QFileDialog, QLabel, QMessageBox, QDialog, QDialogButtonBox,\n",
    "                             QStackedWidget)\n",
    "# Imports de QGeral e QGraficos (Ignorados na explicação do modelo)\n",
    "from PyQt5.QtCore import Qt, QPoint, QBuffer, QIODevice\n",
    "from PyQt5.QtGui import QPixmap, QIcon, QPainter, QPen, QImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a6822",
   "metadata": {},
   "source": [
    "### 2. Arquitetura do Modelo (Perceptron)\n",
    "\n",
    "Antes de qualquer interface aparecer, o script prepara a IA. Ele precisa recriar a arquitetura do Perceptron e carregar o \"cérebro\" treinado (.pth).\n",
    "\n",
    "* **Define a Arquitetura:** O script define exatamente a mesma classe Perceptron que existe no script de treino.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitetura do Modelo\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Entrada: 28*28 = 784 pixels. Saída: 1 logit.\n",
    "        self.fc = nn.Linear(28*28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dcb85",
   "metadata": {},
   "source": [
    "### 3. Carregamento do Modelo\n",
    "\n",
    "*  **`device`**: Define onde o PyTorch executará os cálculos.\n",
    "*  **`model.load_state_dict(...)`**: Carrega os \"conhecimento\" do arquivo `.pth` para a arquitetura do modelo.\n",
    "*  **`model.eval()`**: Coloca o modelo em \"modo de avaliação\". Ele informa ao PyTorch que não precisa calcular gradientes para inferência, para deixar o processo mais rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065054cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o Modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Perceptron().to(device)\n",
    "\n",
    "MODEL_FILE = 'perceptron_A_v5.pth'\n",
    "try:\n",
    "    # Carrega os pesos salvos no disco\n",
    "    model.load_state_dict(torch.load(MODEL_FILE))\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: Arquivo '{MODEL_FILE}' não encontrado.\")\n",
    "    print(\"Por favor, rode o script 'treinar_modelo.py' (v5.0) primeiro.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Coloca em modo de inferência\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7ae32",
   "metadata": {},
   "source": [
    "### 4. Transformações de Pré-processamento da Imagem\n",
    "\n",
    "Prepara uma serie de processamento (`data_transform`) que formata qualquer imagem para o padrão 28x28, converte para escala de cinza, inverte as cores (letra branca/fundo preto) e \"achatada\" em um vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformações da Imagem\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), # Converte para cinza\n",
    "    transforms.Resize((28, 28)),                 # Redimensiona para 28x28\n",
    "    transforms.ToTensor(),                       # Converte para Tensor\n",
    "    transforms.Lambda(lambda x: 1.0 - x),        # Inverte as cores\n",
    "    transforms.Lambda(lambda x: x.view(-1))      # Achata o tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ca79c",
   "metadata": {},
   "source": [
    "### 5. Estrutura da Interface\n",
    "\n",
    "*  HomePage (Linha 160): A tela inicial. Permite ao usuário \"Procurar Imagem...\". Quando uma imagem é carregada, o mesmo botão muda para \"Testar Imagem\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46444d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Página Inicial\n",
    "class HomePage(QWidget):\n",
    "    # ...\n",
    "    def on_action_click(self):\n",
    "        if self.btn_action.text() == 'Procurar Imagem...':\n",
    "            # Carrega a imagem\n",
    "        else:\n",
    "            # Testa a imagem\n",
    "            self.parent_app.run_test(self.image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51fb9c",
   "metadata": {},
   "source": [
    "* DrawingPage (Linha 101): A tela de desenho. Ela fornece um \"canvas\" branco onde o usuário pode desenhar com o mouse. Ao clicar em \"Testar Desenho\", ela converte o desenho em uma imagem e a envia para o teste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524aebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Página de Desenho\n",
    "class DrawingPage(QWidget):\n",
    "    \n",
    "    def mouseMoveEvent(self, event):\n",
    "        # Lógica para desenhar na tela (pixmap)\n",
    "\n",
    "    def test_drawing(self):\n",
    "        # Converte o pixmap para imagem e chama o run_test\n",
    "        self.parent_app.run_test(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834ae38",
   "metadata": {},
   "source": [
    "### 6. Cérebro da Interação\n",
    "\n",
    "As duas funções mais importantes na classe App, que definem o fluxo principal.\n",
    "\n",
    "* run_test (Linha 300): Esta é a função central de teste. Ela é chamada tanto pela HomePage quanto pela DrawingPage.\n",
    "\n",
    "    * Recebe a imagem processada\n",
    "    * Passa a imagem pelo modelo `(model(image_tensor_to_test))` para obter uma previsão.\n",
    "    * Calcula a probabilidade `(torch.sigmoid)`.\n",
    "    * Cria e exibe a janela de diálogo `(FeedbackDialog)` mostrando o resultado.\n",
    "    * Se o usuário corrigiu o modelo (\"Ele Errou!\"), ele chama a próxima função: `aprender_com_feedback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO CENTRAL\n",
    "def run_test(self, image_tensor_to_test):\n",
    "    # ...\n",
    "    probabilidade = torch.sigmoid(output_raw).item()\n",
    "    # ...\n",
    "    dialog = FeedbackDialog('Resultado', msg, self)\n",
    "    dialog.exec_() \n",
    "\n",
    "    # LÓGICA DE APRENDIZADO\n",
    "    if dialog.feedback == 'errado':\n",
    "        # ...\n",
    "        self.aprender_com_feedback(image_tensor_to_test, label_correta, show_thank_you_message=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a679e1e",
   "metadata": {},
   "source": [
    "* `aprender_com_feedback` (Linha 352): Esta é a função que realiza o Aprendizado Contínuo.\n",
    "\n",
    "    * Coloca o modelo em modo de treino `(model.train()).`\n",
    "    * Realiza o passo de aprendizado/backpropagation.\n",
    "    * Coloca o modelo de volta em modo de avaliação `(model.eval())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48907ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de Aprendizado\n",
    "def aprender_com_feedback(self, image_tensor, label_correta, show_thank_you_message):\n",
    "    model.train() # Coloca em modo de treino\n",
    "    optimizer.zero_grad()\n",
    "    # ...\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval() # Volta ao modo de avaliação\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_FILE) # Salva o modelo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
